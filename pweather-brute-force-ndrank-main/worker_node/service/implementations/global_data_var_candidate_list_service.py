import datetime
import logging, xarray
from typing import Any, Dict, Iterable, Iterator, List, Optional, Set, Tuple, Union, cast
import numpy as np
import numpy.typing as npt
from correlation_functions.main_structure import CorrelationFunction
from repository.repository_collection import RepositoryCollection
from repository.repository_layer import RepositoryMetadata
from service.data_types import CandidateContainer, CandidateListManager, InputIterator, ResultContainer
from service.implementations.brute_force_top_n_service import BruteForceTopNService
from service.service_main_structure import HeuristicResult, RequestParameters
from service.constants import DATA_VAR_CANDIDATE_LIST_SERVICE
from auxiliar.component_injector import component_injector

"""
NOTE: it assumes that each worker node has access to all 
variables of the dataset for that specific time instance
"""


@component_injector.inject_service(DATA_VAR_CANDIDATE_LIST_SERVICE)
class DataVarCandidateListService(BruteForceTopNService):
    """Service that calculates the similarity values by 
    creating a list of candidates first and then only takes the best
    available candidates. 

    The candidate is generated by only calculating the similarity for a small number
    of data variables and then deriving what could be the possible interval of values
    for the final similarity value
    """
    def uses_global_candidates(self) -> bool:
        return True

    def _merge_iterator_with_list(self, candidate_list_iterator: Iterator[np.datetime64], 
        partial_list: Iterable[np.datetime64]) -> Iterator[HeuristicResult]:
        
        for date in partial_list:
            yield HeuristicResult(str(date),0.0)
        
        for date in candidate_list_iterator:
            yield HeuristicResult(str(date),0.0)


    def execute_search_for_candidates(self, file_paths: Dict[str,List[str]], request_parameters: RequestParameters, 
        corr_function: CorrelationFunction, num_results:Optional[int] = None) -> Tuple[Dict[np.datetime64, CandidateContainer], int]:
        """

        Args:
            file_paths (List[str]): _description_
            request_parameters (RequestParameters): _description_
            corr_function (CorrelationFunction): _description_
            num_results (Optional[int], optional): _description_. Defaults to None.

        Returns:
            Tuple[Dict[str, ResultContainer], int]: _description_
        """
        """
        The search process using a list of candidates requires the usage of two search
        processes. The first with a full brute force search on a partial selection of variables
        and the second on specific timestamps on all dimensions.

        This can be organized into the following steps:
        1) create an InputIterator with the right parameters
        2) obtain the required timestamps to search in
        3) calculate the possible range for each timestamp
        4) some values it will not be possible to get the
        """
        if request_parameters.search_data_var is None:
            raise ValueError("This service requires the selection of one variable")
        
        if num_results is None or num_results <= 0:
            raise ValueError("This service requires a limited number of results")
        
        if request_parameters.selection_data_vars is None:
            raise ValueError("DataVarCandidateListService requires data variable selection")

        search_data_vars: List[str] = request_parameters.search_data_var
        candidates_temp_holder: Dict[np.datetime64, CandidateContainer] = {}
        candidate_list: CandidateListManager = CandidateListManager(corr_function,num_results)
        selection_data_vars: List[str] = request_parameters.selection_data_vars

        repo_subset: RepositoryCollection = \
            self._repositories.get_subsection_repositories(request_parameters.search_data_var)

        logging.info("Opening input files")
        input_size: int
        input_iterator_collection: Dict[str,InputIterator] 
        input_iterator_collection, input_size = \
            self._open_input_as_dataset_with_multiple_vars(file_paths, request_parameters)

        #the first step is to iterate all existing repositories to find out which have
        # the desired data variables
        for repository in repo_subset.repositories:
            metadata: RepositoryMetadata = repository.get_metadata()
            step_variation: np.timedelta64 = np.timedelta64(int(repo_subset.step_variation),'ns')
            #the datasets have two time dimensions:
            # - one that has a single timestamp (usually the first date of the given file)
            # - another will all step values of the existing time dimension
            # in order to obtain on actual timestamp, it is necessary to sum the the first
            #timestamp together with the used step value
            # This is valid for the data that has been processed
            step_values: npt.NDArray[np.timedelta64]
            time_date: Union[np.datetime64, npt.NDArray[np.datetime64]] #only a Union since can return both kinds
                                                                        #with .values attribute (even tho only the
                                                                        #np.datetime64 is relevant for this context)
            # from this point, then a sequencial iteration of each file is done one by one
            for dataset_pair in repository.get_dataset():
                logging.info("Searching file " + dataset_pair[0])
                step_values = dataset_pair[1].coords[metadata.time_variation_dim].values
                time_date = dataset_pair[1].coords[metadata.time_initial_dim].values
                iterable: bool = True

                if not isinstance(step_values, Iterable):
                    step_values = cast(npt.NDArray[np.timedelta64],np.array([step_values]))
                    iterable = False

                for step in step_values:
                    if metadata.time_gap_container.is_gap(time_date + step, search_data_vars,request_parameters.search_hours):
                        continue

                    params: Dict[str, Any] = {}
                    params[metadata.time_variation_dim] = step
                    dataset_section: xarray.Dataset

                    if iterable:
                        dataset_section = dataset_pair[1].sel(params)
                    else:
                        dataset_section = dataset_pair[1]

                    for var in selection_data_vars:
                        key: np.datetime64 = time_date + step
                        best_val: float = 0.0
                        worst_val: float = 0.0
                        if var in metadata.data_vars:
                            input_iterator: InputIterator = input_iterator_collection[var]

                            for input_tuple in input_iterator.iterate():
                                input_ts: xarray.Dataset = input_tuple[0]
                                input_interval: int = input_tuple[1]
                                key -= step_variation * input_interval
                                sim_val_raw: float = 0.0

                                data_array_section = dataset_section[[var]].to_array()
                                input_array = input_ts[var]
                                sim_val_raw += corr_function.calculate(data_array_section, input_array, metadata, var)

                                best_val = sim_val_raw / len(search_data_vars)
                                worst_val = sim_val_raw / len(search_data_vars)

                                if not key in candidates_temp_holder:
                                    candidates_temp_holder[key] = CandidateContainer(best_val, worst_val,[var])
                                else:
                                    candidates_temp_holder[key].add_value(best_val, worst_val,[var])

                                key -= step_variation
                                while metadata.time_gap_container.is_gap(key, search_data_vars, request_parameters.search_hours):
                                    key -= step_variation
        
        #after all values from the evaluated portions have been calculated, the missing values from the other
        #variables have to be predicted and added to the existing values
        num_vars_used: int = len(selection_data_vars)
        num_vars: int = len(request_parameters.search_data_var)
        missing_vars: int = num_vars - num_vars_used
        best_values: float = (corr_function.max_value / num_vars) * missing_vars
        worst_values: float = (corr_function.min_value / num_vars) * missing_vars

        keys: List[np.datetime64] = list(candidates_temp_holder.keys())
        for timestamp in keys:
            count: int = candidates_temp_holder[timestamp].sum_counter
            candidates_temp_holder[timestamp].add_value_without_increasing_counter(best_values*count,worst_values*count)
            if candidates_temp_holder[timestamp].sum_counter == input_size:
                candidates_temp_holder[timestamp].set_as_final()
                candidate_list.add_value(timestamp, candidates_temp_holder[timestamp])
                del candidates_temp_holder[timestamp]

        return {**candidate_list.to_dict(), **candidates_temp_holder}, input_size

    def execute_search_on_ts(self, result_iterator: Iterator[HeuristicResult], file_paths: Dict[str,List[str]], 
        request_parameters: RequestParameters, corr_function: CorrelationFunction,
        num_results:Optional[int] = None) -> Tuple[Dict[str, ResultContainer], int]:
        return super().execute_search_on_ts(result_iterator, file_paths, request_parameters, corr_function,num_results)
        