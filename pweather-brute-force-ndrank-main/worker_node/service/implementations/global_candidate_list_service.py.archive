import logging, xarray
from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Union, cast
import numpy as np
import numpy.typing as npt
from correlation_functions.correlation_statistics import CorrelationStatistics
from correlation_functions.main_structure import CorrelationFunction
from correlation_functions.statistics_collection import StatisticsCollection
from repository.repository_collection import RepositoryCollection
from repository.repository_layer import RepositoryLayer, RepositoryMetadata
from service.data_types import CandidateContainer, CandidateListManager, InputIterator, ResultContainer
from service.implementations.brute_force_top_n_service import BruteForceTopNService
from service.service_main_structure import HeuristicResult, RequestParameters
from service.constants import CANDIDATE_LIST_SERVICE
from auxiliar.component_injector import component_injector
from auxiliar.xarray_aux import open_dataset_with_file_name

@component_injector.inject_service(CANDIDATE_LIST_SERVICE)
class CandidateListService(BruteForceTopNService):

    def _setup_correlation_function_statistics(self,skip_functions:List[str]) -> None:
        """Iterates the whole dataset to allow the creation of
        the required stats in order to generate the candidate <
        lists
        """
        metadata: RepositoryMetadata = self._repository.get_metadata()

        for dataset_part in self._repository.get_dataset():
            dataset: xarray.Dataset = dataset_part[1]
            var_time: Union[npt.NDArray[np.timedelta64],np.timedelta64] = \
                dataset.coords[metadata.time_variation_dim].values
            init_time: np.datetime64 = \
                dataset.coords[metadata.time_initial_dim].values #type: ignore
            iterable: bool = True
            datetime: np.datetime64

            if not isinstance(var_time, Iterable):
                var_time = cast(npt.NDArray[np.timedelta64],np.array([var_time]))
                iterable = False

            for step in var_time:
                params: Dict[str, Any] = {}
                params[metadata.time_variation_dim] = step
                dataset_section: xarray.Dataset

                if iterable:
                    dataset_section = dataset.sel(params)
                else:
                    dataset_section = dataset
                
                datetime = init_time + step
                self._statistics_collection.insert_statistic(datetime,dataset_section,metadata,skip_functions)

    def __init__(self, repositories: List[RepositoryLayer]) -> None:
        super().__init__(repositories)
        self._statistics_collection: StatisticsCollection = StatisticsCollection()
        logging.info("Gathering required statistics for list of candidates, stand by...")
        self._setup_correlation_function_statistics(skip_functions)
        self._statistics_collection.print_statistics()
        logging.info("Statistics required for list of candidates gathered with success")

    def _open_input_as_dataset_with_partial_params_multiple_vars(self, files: Dict[str,List[str]], 
        request_params: RequestParameters, corr_function: CorrelationFunction, selection_params: Dict[str,Any],
        used_data_vars: List[str], used_data_var: str) -> Tuple[Dict[str, InputIterator],int]:
        res: Dict[str, InputIterator] = {}
        input_size: Optional[int] = None
        for var in files:
            res[var] = self._open_input_as_dataset_with_partial_parameters(files[var], 
            request_params, corr_function, selection_params, used_data_vars, used_data_var)
            if input_size is None:
                input_size = res[var].size
            else:
                if input_size != res[var].size:
                    raise ValueError("One of the inputs has a different size")
        if input_size is None:
            raise ValueError("There must be at least one Input list")
        return (res, input_size)


    def _open_input_as_dataset_with_partial_parameters(self, file_paths: List[str], 
        request_params: RequestParameters, corr_function: CorrelationFunction, 
        selection_params: Dict[str,Any], used_data_vars: List[str], used_data_var) -> InputIterator:
        """Returns the received input as a single iterator already selected with the
        given parameters

        Args:
            file_paths (List[str]): paths of the input files
            request_params (RequestParameters): parameters given from the request
            corr_function (CorrelationFunction): used correlation function
            selection_params (Dict[str,Any]): parameter used to decide in which variables 
            to calculate the similiarty
            used_data_vars (str): data variable being used for search process
            used_data_var (str): data var being used for the specific search

        Returns:
            InputIterator: resulting Iterator
        """
        aux: List[xarray.Dataset] = []

        for path in file_paths:
            aux.append(
                self._select_dataset_datavar(
                    request_params,open_dataset_with_file_name(path)
                    )
                )
        return InputIterator(aux, self._repositories.get_metadata_by_data_var(used_data_var),
            used_data_vars,request_params.input_step_difference,corr_function,selection_params)

    def uses_global_candidates(self) -> bool:
        return True

    def _select_dataset_datavar(self, request_parameters: RequestParameters,
        dataset: xarray.Dataset) -> xarray.Dataset:
        """Verifies if the request parameters request if the parameters should be
        search in one data var only or not and returns the right dataset

        Args:
            request_parameters (RequestParameters): parameters received from the request
            dataset (xarray.Dataset): dataset to select

        Returns:
            xarray.Dataset: the correct dataset
        """
        if not request_parameters.search_data_var is None:
            try:
                return dataset[request_parameters.search_data_var]
            except KeyError:
                raise ValueError("Data variable '" + str(request_parameters.search_data_var) + \
                                 "' does not exist in file")
        else:
            return dataset

    def _merge_iterator_with_list(self, candidate_list_iterator: Iterator[np.datetime64], 
        partial_list: Iterable[np.datetime64]) -> Iterator[HeuristicResult]:
        
        for date in partial_list:
            yield HeuristicResult(str(date),0.0)
        
        for date in candidate_list_iterator:
            yield HeuristicResult(str(date),0.0)


    def execute_search_for_candidates(self, file_paths: Dict[str,List[str]], request_parameters: RequestParameters, 
        corr_function: CorrelationFunction, num_results: Optional[int] = None) -> Tuple[Dict[np.datetime64, CandidateContainer], int]:
        """

        Args:
            file_paths (Dict[str,List[str]]): _description_
            request_parameters (RequestParameters): _description_
            corr_function (CorrelationFunction): _description_
            num_results (Optional[int], optional): _description_. Defaults to None.

        Returns:
            Tuple[Dict[str, ResultContainer], int]: _description_
        """
        """
        The search process using a list of candidates requires the usage of two search
        processes. The first with a full brute force search on a partial selection of variables
        and the second on specific timestamps on all dimensions.

        This can be organized into the following steps:
        1) create an InputIterator with the right parameters
        2) obtain the required timestamps to search in
        3) calculate the possible range for each timestamp
        4) some values it will not be possible to get the
        """
        if request_parameters.dataset_selection_parameters is None:
            raise ValueError("Request must provide the parameters for the dataset selection")
        if request_parameters.search_data_var is None:
            raise ValueError("Brute force service requires for the data variable to be defined")

        selection_params: Dict[str, Any] = {}
        for elem in request_parameters.dataset_selection_parameters:
            selection_params[elem.name] = slice(elem.max,elem.min)
        
        logging.debug(selection_params)

        repo_subset: RepositoryCollection = \
            self._repositories.get_subsection_repositories(request_parameters.search_data_var)


        if request_parameters.search_data_var is None:
            raise ValueError("This service requires the selection of one variable")
        
        if num_results is None or num_results <= 0:
            raise ValueError("This service requires a limited number of results")
        
        search_data_vars: List[str] = request_parameters.search_data_var
        candidates_temp_holder: Dict[np.datetime64, CandidateContainer] = {}
        candidate_list: CandidateListManager = CandidateListManager(corr_function,num_results)

        logging.info("Opening input files")
        input_size: int
        input_iterator_collection: Dict[str,InputIterator] 
        input_iterator_collection, input_size = \
            self._open_input_as_dataset_with_partial_params_multiple_vars(
                file_paths, request_parameters,corr_function, selection_params,
                search_data_vars) 

        for var in search_data_vars:
            repository: RepositoryLayer = repo_subset.get_repository_by_data_var(var)
            metadata: RepositoryMetadata = repository.get_metadata()
            step_variation: np.timedelta64 = np.timedelta64(int(repo_subset.step_variation),'ns')
            #the datasets have two time dimensions:
            # - one that has a single timestamp (usually the first date of the given file)
            # - another will all step values of the existing time dimension
            # in order to obtain on actual timestamp, it is necessary to sum the the first
            #timestamp together with the used step value
            # This is valid for the data that has been processed
            step_values: npt.NDArray[np.timedelta64]
            time_date: Union[np.datetime64, npt.NDArray[np.datetime64]] #only a Union since can return both kinds
                                                                        #with .values attribute (even tho only the
                                                                        #np.datetime64 is relevant for this context)

            for dataset_pair in repository.get_dataset():
                logging.info("Searching file " + dataset_pair[0])
                step_values = dataset_pair[1].coords[metadata.time_variation_dim].values
                time_date = dataset_pair[1].coords[metadata.time_initial_dim].values
                iterable: bool = True

                if not isinstance(step_values, Iterable):
                    step_values = cast(npt.NDArray[np.timedelta64],np.array([step_values]))
                    iterable = False

                for step in step_values:
                    if repo_subset.is_gap(time_date + step, search_data_vars,request_parameters.search_hours):
                        continue

                    params: Dict[str, Any] = {}
                    params[metadata.time_variation_dim] = step
                    dataset_section: xarray.Dataset

                    if iterable:
                        dataset_section = dataset_pair[1].sel(params)
                    else:
                        dataset_section = dataset_pair[1]

                    #verify if only one parameter should be searched
                    dataset_section = \
                        self._select_dataset_datavar(
                            request_parameters, 
                            dataset_section).sel(selection_params)

                    key: np.datetime64 = time_date + step
                    corr_stats: CorrelationStatistics
                    input_iterator: InputIterator = input_iterator_collection[var]

                    for input_stats_ts in input_iterator.iterate_with_statistics():
                        best_val: float = 0.0
                        worst_val: float = 0.0
                        key -= step_variation * input_stats_ts[2]

                        best_val_aux: float = 0.0; worst_val_aux: float = 0.0

                        corr_stats = self._statistics_collection.get_statistics(
                            corr_function.corr_func_name,var,time_date + step)
                        best_val_aux, worst_val_aux = \
                            corr_function.calculate_partial_value(
                                input_stats_ts[0][var],dataset_section[var], 
                                input_stats_ts[1][var],corr_stats, 
                                selection_params,metadata,var)
                        best_val += best_val_aux
                        worst_val += worst_val_aux

                        #TODO: make sure this change is reflected all over the code
                        #best_val  /= len(search_data_vars)
                        #worst_val /= len(search_data_vars)

                        if not key in candidates_temp_holder:
                            candidates_temp_holder[key] = CandidateContainer(best_val, worst_val)
                        else:
                            candidates_temp_holder[key].add_value(best_val, worst_val)

                        if candidates_temp_holder[key].sum_counter == input_size:
                            candidates_temp_holder[key].set_as_final()
                            candidate_list.add_value(key, candidates_temp_holder[key])
                            del candidates_temp_holder[key]

                        key -= step_variation
                        while repo_subset.is_gap(key, search_data_vars, request_parameters.search_hours):
                            key -= step_variation

        return {**candidate_list.to_dict(), **candidates_temp_holder}, input_size

    def execute_search_on_ts(self, result_iterator: Iterator[HeuristicResult], file_paths: Dict[str,List[str]], request_parameters: RequestParameters, 
        corr_function: CorrelationFunction, num_results:Optional[int] = None) -> Tuple[Dict[str, ResultContainer], int]:
        return super().execute_search_on_ts(result_iterator, file_paths, request_parameters, corr_function,num_results)
        